{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232cb93a",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42c7b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation d'un appelle √† une API m√©t√©o\n",
    "def get_weather(city):\n",
    "    temp = 25\n",
    "    wind = 30\n",
    "    return f\"Il fait {temp}¬∞C avec un vent √† {wind}‚ÄØkm/h.\"\n",
    "\n",
    "def get_time():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ce8acfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user is asking for current weather information in a specific location, which requires using an appropriate tool.\n",
      "Action: get_weather\n",
      "Action Input: city=Paris\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIl fait 25¬∞C avec un vent √† 30‚ÄØkm/h.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now have the requested weather details to provide as my final answer.\n",
      "\n",
      "Final Answer: La m√©t√©o actuelle √† Paris est de 25¬∞C avec un vent √† une vitesse de 30 km/h.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "La m√©t√©o actuelle √† Paris est de 25¬∞C avec un vent √† une vitesse de 30 km/h.\n"
     ]
    }
   ],
   "source": [
    "# On cr√©e le Tool LangChain\n",
    "weather_tool = Tool.from_function(\n",
    "    func=get_weather,\n",
    "    name=\"get_weather\",\n",
    "    description=\"Donne la m√©t√©o actuelle pour une ville sp√©cifi√©e.\"\n",
    ")\n",
    "\n",
    "# On instancie ton mod√®le Ollama\n",
    "#llm = ChatOllama(model=\"gemma3n:e2b\")\n",
    "#llm = ChatOllama(model=\"qwen2.5:3b\")\n",
    "llm = ChatOllama(model=\"phi4-mini:latest\")\n",
    "\n",
    "# On initialise l'agent\n",
    "agent = initialize_agent(\n",
    "    tools=[weather_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    agent_kwargs={\"prefix\": \"\"\"Si tu peux r√©pondre directement √† la question (sans appeler un outil), fais-le. \n",
    "N‚Äôutilise un outil que si c‚Äôest strictement n√©cessaire. \n",
    "Respecte scrupuleusement ce format pour les outils :\n",
    "\n",
    "Thought: Raisonne bri√®vement\n",
    "Action: nom_de_l_outil\n",
    "Action Input: valeur\"\"\"}\n",
    ")\n",
    "\n",
    "# On pose une question √† l'agent\n",
    "result = agent.run(\"Peux-tu me dire la m√©t√©o actuelle √† Paris‚ÄØ?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "423ee0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe calculation \"2 + 2\" is a basic arithmetic problem with an objective and unambiguous correct numerical result that does not require any tools or external data sources. I can provide this simple mathematical fact directly.\n",
      "\n",
      "Final Answer: The sum of 2 plus 2 equals 4.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The sum of 2 plus 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "result = agent.run(\"Combien font 2+2‚ÄØ?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b20e0c",
   "metadata": {},
   "source": [
    "Analyse :\n",
    "Beaucoup de probl√©mes soit il utilise trop l'outils soit il y a des erreurs de parsing. Pas tr√®s stable. Ou alors la r√©ponse est en anglais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d1ace",
   "metadata": {},
   "source": [
    "# A la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "53c7c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_tools = {\n",
    "    \"get_weather\": {\n",
    "        \"description\": \"Donne la m√©t√©o actuelle pour une ville sp√©cifi√©e.\",\n",
    "        \"format\": \"\"\"{\"name\": \"get_weather\", \"params\": [\"ville\"]}\"\"\",\n",
    "        \"tool_function\": get_weather\n",
    "    },\n",
    "    \"get_time\": {\n",
    "        \"description\": \"Donne l'heure actuelle.\",\n",
    "        \"format\": \"\"\"{\"name\": \"get_time\", \"params\": []}\"\"\",\n",
    "        \"tool_function\": get_time\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81bc47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_llm_tool(question, available_tools):\n",
    "    tools_description = \"\"\n",
    "    for tool_name, tool_info in available_tools.items():\n",
    "        tools_description += f\"- Nom: {tool_name}\\n\"\n",
    "        tools_description += f\"  Description: {tool_info['description']}\\n\"\n",
    "        tools_description += f\"  Format: {tool_info['format']}\\n\\n\"\n",
    "    \n",
    "    formatted_prompt = f\"\"\"Tu es un assistant intelligent qui doit analyser les questions des utilisateurs et d√©terminer si elles n√©cessitent l'utilisation d'un outil sp√©cifique.\n",
    "\n",
    "OUTILS DISPONIBLES:\n",
    "{tools_description}\n",
    "\n",
    "QUESTION DE L'UTILISATEUR:\n",
    "{question}\n",
    "\n",
    "    INSTRUCTIONS:\n",
    "    1. Analyse la question de l'utilisateur\n",
    "    2. D√©termine si elle n√©cessite l'utilisation d'un des outils disponibles\n",
    "    3. R√©ponds UNIQUEMENT par:\n",
    "    - Format JSON: {{\"tools\": [{{\"name\": \"nom_outil1\", \"params\": [\"param√®tre1\", \"param√®tre2\",...]}},{{\"name\": \"nom_outil2\", \"params\": [\"param√®tre1\",...]}}]}} si des outils sont n√©cessaires\n",
    "    - Format JSON: {{\"tools\": \"NONE\"}} si tu peux r√©pondre directement sans outil\n",
    "\n",
    "    EXEMPLES:\n",
    "    - Question: \"Quel temps fait-il √† Paris ?\" ‚Üí R√©ponse: {{\"tools\": [{{\"name\": \"get_weather\", \"params\": [\"Paris\"]}}]}}\n",
    "    - Question: \"Quelle heure est-il ?\" ‚Üí R√©ponse: {{\"tools\": [{{\"name\": \"get_time\", \"params\": []}}]}}\n",
    "    - Question: \"Dis-moi l'heure et la m√©t√©o √† Lyon\" ‚Üí R√©ponse: {{\"tools\": [{{\"name\": \"get_time\", \"params\": []}}, {{\"name\": \"get_weather\", \"params\": [\"Lyon\"]}}]}}\n",
    "    - Question: \"Combien font 2+2 ?\" ‚Üí R√©ponse: {{\"tools\": \"NONE\"}}\n",
    "\n",
    "    R√âPONSE:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"gemma3n:e2b\",\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]\n",
    "    )\n",
    "\n",
    "    response_content = response['message']['content']\n",
    "    final_answer = re.sub(r'<think>.*?</think>',\n",
    "                          '',\n",
    "                          response_content,\n",
    "                          flags=re.DOTALL).strip()\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7aa3e70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"tools\": [{\"name\": \"get_weather\", \"params\": [\"Tallard\"]},{\"name\": \"get_time\", \"params\": []}]}'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = ollama_llm_tool(\"Peux-tu me dire la m√©t√©o actuelle √† Tallard et l'heur actuelle ?\", available_tools)\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e03d565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"tools\": \"NONE\"}'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = ollama_llm_tool(\"Tu est plutot chien ou chat ?\", available_tools)\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tool_response(response):\n",
    "    first_brace = response.find('{')\n",
    "    last_brace = response.rfind('}')\n",
    "\n",
    "    # Si on trouve les deux accolades\n",
    "    if first_brace != -1 and last_brace != -1 and first_brace < last_brace:\n",
    "        json_str = response[first_brace:last_brace + 1]\n",
    "        print(f\"JSON extrait: {json_str}\")\n",
    "        \n",
    "        try:\n",
    "            # Tenter de parser le JSON\n",
    "            parsed = json.loads(json_str)\n",
    "            print(f\"JSON pars√© avec succ√®s: {parsed}\")\n",
    "        \n",
    "        # Si il y a une erreur pour ne pas couper le programme on return {'tools': 'NONE'}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Erreur de parsing JSON: {e}\")\n",
    "            return {'tools': 'NONE'}\n",
    "\n",
    "    return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4a9bfe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON extrait: {\"tools\": [{\"name\": \"get_weather\", \"params\": [\"Tallard\"]},{\"name\": \"get_time\", \"params\": []}]}\n",
      "JSON pars√© avec succ√®s: {'tools': [{'name': 'get_weather', 'params': ['Tallard']}, {'name': 'get_time', 'params': []}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tools': [{'name': 'get_weather', 'params': ['Tallard']},\n",
       "  {'name': 'get_time', 'params': []}]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_to_call = parse_tool_response(r1)\n",
    "tools_to_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6498cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tools(tools_to_call, available_tools):\n",
    "    if tools_to_call.get(\"tools\") == \"NONE\":\n",
    "        return None\n",
    "\n",
    "    results = []\n",
    "    for tool in tools_to_call.get(\"tools\", []):\n",
    "        tool_name = tool[\"name\"]\n",
    "        params = tool[\"params\"]\n",
    "        \n",
    "        if tool_name in available_tools:\n",
    "            tool_info = available_tools[tool_name]\n",
    "            tool_function = tool_info[\"tool_function\"]\n",
    "            tool_description = tool_info[\"description\"]\n",
    "            \n",
    "            result = tool_function(*params)\n",
    "            results.append({\n",
    "                \"tool_name\": tool_name,\n",
    "                \"description\": tool_description,\n",
    "                \"params\": params,\n",
    "                \"result\": result\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Outil {tool_name} non reconnu.\")\n",
    "            results.append({\n",
    "                \"tool_name\": tool_name,\n",
    "                \"description\": \"Outil non trouv√©\",\n",
    "                \"params\": params,\n",
    "                \"result\": f\"Erreur: Outil '{tool_name}' non disponible\"\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "29eb14d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_name': 'get_weather',\n",
       "  'description': 'Donne la m√©t√©o actuelle pour une ville sp√©cifi√©e.',\n",
       "  'params': ['Tallard'],\n",
       "  'result': 'Il fait 25¬∞C avec un vent √† 30\\u202fkm/h.'},\n",
       " {'tool_name': 'get_time',\n",
       "  'description': \"Donne l'heure actuelle.\",\n",
       "  'params': [],\n",
       "  'result': '11:50:13'}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_tools(tools_to_call, available_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "944163fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_agent(question, available_tools):\n",
    "\n",
    "    tools_to_call = parse_tool_response(ollama_llm_tool(question, available_tools))\n",
    "    tools_results = execute_tools(tools_to_call, available_tools)\n",
    "\n",
    "    if tools_results is None:\n",
    "        formatted_prompt = f\"\"\"Tu es un assistant intelligent. R√©ponds √† cette question :\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "R√©ponds de mani√®re naturelle et conversationnelle en fran√ßais.\"\"\"\n",
    "    \n",
    "    else:\n",
    "        # Formatter les r√©sultats des outils\n",
    "        tools_summary = \"\"\n",
    "        for tool_result in tools_results:\n",
    "            tool_name = tool_result[\"tool_name\"]\n",
    "            description = tool_result[\"description\"]\n",
    "            params = tool_result[\"params\"]\n",
    "            result = tool_result[\"result\"]\n",
    "            \n",
    "            tools_summary += f\"- Outil: {tool_name} ({description})\\n\"\n",
    "            tools_summary += f\"  Param√®tres: {params}\\n\"\n",
    "            tools_summary += f\"  R√©sultat: {result}\\n\\n\"\n",
    "        \n",
    "        formatted_prompt = f\"\"\"Tu es un assistant intelligent. L'utilisateur a pos√© une question et j'ai utilis√© des outils pour obtenir des informations.\n",
    "\n",
    "QUESTION DE L'UTILISATEUR:\n",
    "{question}\n",
    "\n",
    "R√âSULTATS DES OUTILS:\n",
    "{tools_summary}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Utilise les r√©sultats des outils pour r√©pondre √† la question de l'utilisateur\n",
    "- R√©ponds de mani√®re naturelle et conversationnelle en fran√ßais\n",
    "- Synth√©tise les informations de mani√®re claire et utile\n",
    "- Ne mentionne pas les noms techniques des outils, parle naturellement\n",
    "\n",
    "R√âPONSE:\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"gemma3n:e2b\",\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]\n",
    "    )\n",
    "\n",
    "    response_content = response['message']['content']\n",
    "    final_answer = re.sub(r'<think>.*?</think>',\n",
    "                          '',\n",
    "                          response_content,\n",
    "                          flags=re.DOTALL).strip()\n",
    "    \n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fd9c6f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON extrait: {\"tools\": \"NONE\"}\n",
      "JSON pars√© avec succ√®s: {'tools': 'NONE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Salut ! üòä\\n\\nLa France a environ 68 millions d'habitants en ce moment. C'est une population assez importante, et elle est r√©partie sur tout le territoire ! \\n\\nTu veux savoir quelque chose d'autre sur la France ? Par exemple, la population des grandes villes, ou peut-√™tre la r√©partition g√©ographique ? Je peux essayer de t'aider ! üòâ\""
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_llm_agent(\"Combien d'habitant poss√®de la france ?\", available_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eacb12f",
   "metadata": {},
   "source": [
    "R√©sultat : Bien meilleur ! M√™me s'il n'y a pas de callback √† r√©p√©tition comme avec LangChain, le mod√®le tourne moins en boucle et se concentre davantage sur ses t√¢ches. Plus pr√©cis, en tout cas avec un petit mod√®le."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
